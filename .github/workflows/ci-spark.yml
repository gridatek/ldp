name: Spark CI

on:
  push:
    branches: [main]
    paths:
      - 'spark/**'
      - 'docker/spark/**'
      - '.github/workflows/ci-spark.yml'
  pull_request:
    branches: [main]
    paths:
      - 'spark/**'
      - 'docker/spark/**'
  workflow_dispatch:

jobs:
  test-spark:
    name: Test Spark Jobs
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.12
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-spark-pip-${{ hashFiles('docker/spark/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-spark-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-cov
          pip install -r docker/spark/requirements.txt || true

      - name: Lint Spark code with flake8
        run: |
          pip install flake8
          flake8 spark/ --count --select=E9,F63,F7,F82 --show-source --statistics
          flake8 spark/ --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics

      - name: Test Spark transformations
        run: |
          export PYTHONPATH="${PYTHONPATH}:${PWD}/spark"
          pytest spark/tests/ -v --tb=short || true

  build-image:
    name: Build Spark Docker Image
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build Spark image
        run: |
          docker build -f docker/spark/Dockerfile -t ldp-spark:test .
